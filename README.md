# PianoGPT: A Generative Transformer for Symbolic Music


PianoGPT is a custom-built, decoder-only Transformer designed to generate coherent musical compositions. By leveraging the **REMI (Revamped MIDI)** tokenization scheme and a multi-head self-attention architecture, the model learns the long-range dependencies of melody, harmony, and rhythm.

---

## üöÄ Features

* **Causal Self-Attention:** Implements a 4-head attention mechanism with a causal mask to ensure temporal integrity.
* **REMI Tokenization:** Supports velocity, tempo, and chord changes, moving beyond simple pitch-based generation.
* **Residual Architecture:** Utilizes skip-connections and Layer Normalization for stable training of deep neural networks.
* **Scalable Design:** Configurable embedding dimensions and block sizes to balance performance and computational efficiency.

---

## üõ†Ô∏è Technical Specifications

| Component | Specification |
| :--- | :--- |
| **Architecture** | Transformer Decoder (4 Layers, 4 Heads) |
| **Embedding Dimension** | 128 |
| **Context Window** | 256 tokens |
| **Optimization** | AdamW ($lr=3e-4$) |
| **Framework** | PyTorch & MidiTok |

---

## üì¶ Installation & Usage

### 1. Clone the repository
```bash
git clone [https://github.com/amirhrafiei/pianogpt.git](https://github.com/amirhrafiei/pianogpt.git)
cd pianogpt
2. Install dependencies
Bash

pip install torch miditok symusic pathlib
3. Train the model
Place your MIDI files in a directory named my_midis/ and run:

Bash

python pianogpt.py
üéµ Hear the Results
You can find pre-generated samples in the /samples directory. These were generated by a model trained on the MAESTRO Dataset (Piano solo).

##üìà Methodology
This project implements the Scaled Dot-Product Attention mechanism from the seminal "Attention Is All You Need" paper. The model was optimized using cross-entropy loss to predict the next token in a sequence, effectively learning the underlying probability distribution of musical structures.

By representing musical events as discrete tokens (REMI), we transform the composition task into a sequence-to-sequence prediction problem, allowing the Transformer to capture both local harmonic patterns and global structural consistency.